{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "window segmentation of edf file",
   "id": "aee50aaeeda850cc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def read_edf(files_train):\n",
    "    time_window = 8\n",
    "    time_step = 4\n",
    "\n",
    "    if os.path.exists('/kaggle/input/mit-chb-processed/signal_samples.npy')&os.path.exists('/kaggle/input/mit-chb-processed/is_sz.npy'):\n",
    "        array_signals=np.load('/kaggle/input/mit-chb-processed/signal_samples.npy')\n",
    "        array_is_sz=np.load('/kaggle/input/mit-chb-processed/is_sz.npy')\n",
    "    else:\n",
    "        p = 0.01\n",
    "        counter = 0\n",
    "        step_window = 0\n",
    "        for temp_f in files_train:\n",
    "            temp_edf =  mne.io.read_raw_edf(temp_f)\n",
    "            temp_labels = temp_edf.ch_names\n",
    "            if sum([any([0 if re.match(c, l) is None else 1 for l in temp_edf.ch_names]) for c in ch_labels])==len(ch_labels):\n",
    "                time_window = 8\n",
    "                time_step = 4\n",
    "                fs = int(1/(temp_edf.times[1]-temp_edf.times[0]))\n",
    "                step_window = time_window*fs\n",
    "                step = time_step*fs\n",
    "\n",
    "                temp_is_sz = np.zeros((temp_edf.n_times,))\n",
    "                if os.path.exists(temp_f+'.seizures'):\n",
    "                    temp_annotation = wfdb.rdann(temp_f, 'seizures')\n",
    "                    for i in range(int(temp_annotation.sample.size/2)):\n",
    "                        temp_is_sz[temp_annotation.sample[i*2]:temp_annotation.sample[i*2+1]]=1\n",
    "                temp_len = temp_edf.n_times\n",
    "\n",
    "                temp_is_sz_ind = np.array(\n",
    "                    [temp_is_sz[i*step:i*step+step_window].sum()/step_window for i in range((temp_len-step_window)//step)]\n",
    "                )\n",
    "\n",
    "                temp_0_sample_size = round(p*np.where(temp_is_sz_ind==0)[0].size)\n",
    "                temp_1_sample_size = np.where(temp_is_sz_ind>0)[0].size\n",
    "\n",
    "                counter = counter + temp_0_sample_size + temp_1_sample_size\n",
    "            temp_edf.close()\n",
    "            return counter, ch_labels, step_window\n",
    "\n",
    "def extract_edf2(files_train, counter, ch_labels, step_window):\n",
    "        array_signals = np.zeros((counter, len(ch_labels), step_window), dtype=np.float32)\n",
    "        array_is_sz = np.zeros(counter, dtype=bool)\n",
    "\n",
    "        counter = 0\n",
    "        for n, temp_f in enumerate(tqdm.tqdm(files_train)):\n",
    "            to_log = 'No. {}: Reading. '.format(n)\n",
    "            temp_edf =  mne.io.read_raw_edf(temp_f)\n",
    "            temp_labels = temp_edf.ch_names\n",
    "            n_label_match = sum([any([0 if re.match(c, l)==None else 1 for l in temp_edf.ch_names]) for c in ch_labels])\n",
    "            if n_label_match==len(ch_labels):\n",
    "                ch_mapping = {sorted([l for l in temp_edf.ch_names if re.match(c, l)!=None ])[0]:c for c in ch_labels}\n",
    "                temp_edf.rename_channels(ch_mapping)\n",
    "                #temp_edf = temp_edf.pick(ch_labels)\n",
    "\n",
    "                temp_is_sz = np.zeros((temp_edf.n_times,))\n",
    "                temp_signals = temp_edf.get_data(picks=ch_labels)*1e6\n",
    "\n",
    "                if os.path.exists(temp_f+'.seizures'):\n",
    "                    to_log = to_log+'sz exists.'\n",
    "                    temp_annotation = wfdb.rdann(temp_f, 'seizures')\n",
    "                    for i in range(int(temp_annotation.sample.size/2)):\n",
    "                        temp_is_sz[temp_annotation.sample[i*2]:temp_annotation.sample[i*2+1]]=1\n",
    "                else:\n",
    "                    to_log = to_log+'No sz.'\n",
    "\n",
    "                temp_len = temp_edf.n_times\n",
    "\n",
    "                time_window = 8\n",
    "                time_step = 4\n",
    "                fs = int(1/(temp_edf.times[1]-temp_edf.times[0]))\n",
    "                step_window = time_window*fs\n",
    "                step = time_step*fs\n",
    "\n",
    "                temp_is_sz_ind = np.array(\n",
    "                    [temp_is_sz[i*step:i*step+step_window].sum()/step_window for i in range((temp_len-step_window)//step)]\n",
    "                )\n",
    "                del temp_is_sz\n",
    "\n",
    "                temp_0_sample_size = round(p*np.where(temp_is_sz_ind==0)[0].size)\n",
    "                temp_1_sample_size = np.where(temp_is_sz_ind>0)[0].size\n",
    "\n",
    "                # sz data\n",
    "                temp_ind = list(np.where(temp_is_sz_ind>0)[0])\n",
    "                for i in temp_ind:\n",
    "                    array_signals[counter, :, :] = temp_signals[:, i*step:i*step+step_window]\n",
    "                    array_is_sz[counter] = True\n",
    "                    counter = counter+1\n",
    "\n",
    "                # no sz data\n",
    "                temp_ind = random.sample(list(np.where(temp_is_sz_ind==0)[0]), temp_0_sample_size)\n",
    "                for i in temp_ind:\n",
    "                    array_signals[counter, :, :] = temp_signals[:, i*step:i*step+step_window]\n",
    "                    array_is_sz[counter] = False\n",
    "                    counter = counter+1\n",
    "\n",
    "                to_log += '{} signals added: {} w/o sz, {} w/ sz.'.format(\n",
    "                    temp_0_sample_size+temp_1_sample_size, temp_0_sample_size, temp_1_sample_size\n",
    "                )\n",
    "                print(\"array_signals\",array_signals.shape, array_signals[:2], \"temp_signals\",temp_signals.shape, temp_signals[:2])\n",
    "\n",
    "            else:\n",
    "                to_log += 'Not appropriate channel labels. Reading skipped.'.format(n)\n",
    "\n",
    "\n",
    "            temp_edf.close()\n",
    "\n",
    "        np.save('signal_samples', array_signals)\n",
    "        np.save('is_sz', array_is_sz)\n"
   ],
   "id": "a9bcb18fd6e85e6e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# same as above but with comments\n",
    "def extract_edf2(folder, n_samples, n_channel_labels, window_size):\n",
    "    if os.path.exists(\" \") and os.path.exists(\" \"):\n",
    "        print(\"File  placeholder name and seizure_labels.npy exists.\")\n",
    "        return\n",
    "    signals_np = np.zeros((n_samples, n_channel_labels, window_size), dtype=np.float32)\n",
    "    labels_np = np.zeros(n_samples, dtype=bool)\n",
    "\n",
    "\n",
    "    # Read  edf, rename file channel to match names from channel labels list. When files have multiple channel names, only first is picked.\n",
    "    for number, file in enumerate(tqdm.tqdm(folder)):\n",
    "        log = f\"Reading file {number} \"\n",
    "        edf_file = mne.io.read_raw_edf(file, preload=False)\n",
    "\n",
    "        n_label_match = sum([any([0 if re.match(ch, ch_name) is None else 1 for ch_name in edf_file.ch_names]) for ch in channel_labels])\n",
    "        if n_label_match == len(channel_labels):\n",
    "            dict_ch_name = {sorted([ch_name for ch_name in edf_file.ch_names if re.match(ch, ch_name) is not None])[0]: ch for ch in channel_labels}\n",
    "            edf_file.rename_channels(dict_ch_name)\n",
    "\n",
    "            # Retrieve EEG (in microvolts) ,  annotations\n",
    "            has_seizure = np.zeros((edf_file.n_times,))\n",
    "            signals_ = edf_file.get_data(picks=channel_labels) * TO_MICROVOLTS\n",
    "\n",
    "            if os.path.exists(file + '.seizures'):\n",
    "                log += \"positive seizure\"\n",
    "                has_annotation = wfdb.rdann(file, 'seizures')\n",
    "                for idx in range(int(has_annotation.sample.size / 2)):\n",
    "                    has_seizure[has_annotation.sample[idx * 2]:has_annotation.sample[idx * 2 + 1]] = 1\n",
    "            else:\n",
    "                log += \"negative seizure\"\n",
    "\n",
    "            # create seizure index, and calculate proportion of signal which shows seizures\n",
    "            sampling_freq = int(1/(edf_file.times[1] - edf_file.times[0]))\n",
    "            print(\"sampling frequency \",sampling_freq)\n",
    "            window_stride = int(sampling_freq * STEP_TIME)\n",
    "            print(f\"Window stride: {window_stride}, Window size: {window_size}\")\n",
    "            is_seizure_idx = np.array([has_seizure[idx * window_stride:idx * window_stride + window_size].sum() / window_size for idx in range((edf_file.n_times - window_size) // window_stride)])\n",
    "\n",
    "            # populate numpy array with EEG , annotation data\n",
    "            noseizure_n_size = round(SEIZURE_PROPORTION * np.where(is_seizure_idx == 0)[0].size)\n",
    "            seizure_n_size = np.where(is_seizure_idx > 0)[0].size\n",
    "\n",
    "\n",
    "            # non seizure data are by far larger than seizure data. To avoid overfitting, and bias, non seizure data is randomly subsampled. This prevents model from being overwhelmed by large non seizure data\n",
    "            count = 0\n",
    "            # no seizure\n",
    "            temp_negative = random.sample(list(np.where(is_seizure_idx == 0)[0]), noseizure_n_size)\n",
    "            for value in temp_negative:\n",
    "                start_idx = value * window_stride\n",
    "                end_idx = start_idx + window_size\n",
    "                segment = signals_[:, start_idx:end_idx]\n",
    "                print(f\"Segment shape: {segment.shape}\")  # Debug print\n",
    "                if segment.shape == (n_channel_labels, window_size):\n",
    "                    signals_np[count, :, :] = segment\n",
    "                    labels_np[count] = False\n",
    "                    count += 1\n",
    "                else:\n",
    "                    print(f\"Skipping segment due to shape mismatch: {segment.shape}\")\n",
    "\n",
    "            # seizure\n",
    "            temp_positive = list(np.where(is_seizure_idx > 0)[0])\n",
    "            for value in temp_positive:\n",
    "                start_idx = value * window_stride\n",
    "                end_idx = start_idx + window_size\n",
    "                segment = signals_[:, start_idx:end_idx]\n",
    "                print(f\"Segment shape: {segment.shape}\")  # Debug print\n",
    "                if segment.shape == (n_channel_labels, window_size):\n",
    "                    signals_np[count, :, :] = segment\n",
    "                    labels_np[count] = True\n",
    "                    count += 1\n",
    "                else:\n",
    "                    print(f\"Skipping segment due to shape mismatch: {segment.shape}\")\n",
    "\n",
    "            print(f\"{noseizure_n_size + seizure_n_size} EEG signals added {seizure_n_size} with, {noseizure_n_size} without seizures\")\n",
    "        else:\n",
    "            print(f\"Unable to read {file}\")\n",
    "\n",
    "        edf_file.close()\n",
    "\n",
    "    np.save('eeg_signals', signals_np)\n",
    "    np.save('seizure_labels', labels_np)"
   ],
   "id": "7ce542783d9ecd3a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "count1, ch_labels1, step_window1 = read_edf(train_files)\n",
    "extract_edf2(train_files, count1, ch_labels1, step_window1)"
   ],
   "id": "c19648a65078925f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "64961d513857433f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Recurrent neural network ( Bidirectional LSTM )",
   "id": "d344a60a72583ec3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import LSTM, Bidirectional, Dropout, Dense\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class RnnModel:\n",
    "    def __init__(self, train_signals, train_labels, test_signals, test_labels):\n",
    "        self.train_signals = train_signals\n",
    "        self.train_labels = train_labels\n",
    "        self.test_signals = test_signals\n",
    "        self.test_labels = test_labels\n",
    "        self.model = None\n",
    "        self.recall = None\n",
    "\n",
    "    def train(self, save_filepath='best_model.keras'):\n",
    "        \"\"\"Train the model and save the best one during training.\"\"\"\n",
    "        # Instantiate sequential API\n",
    "        model = Sequential()\n",
    "\n",
    "        timesteps = self.train_signals.shape[1]\n",
    "        features = self.train_signals.shape[2]\n",
    "\n",
    "        # Add first Bidirectional LSTM layer\n",
    "        model.add(Bidirectional(LSTM(128, return_sequences=True), input_shape=(timesteps, features)))\n",
    "        # Add dropout to prevent overfitting\n",
    "        model.add(Dropout(0.3))\n",
    "\n",
    "        # Add second Bidirectional LSTM layer\n",
    "        model.add(Bidirectional(LSTM(64, return_sequences=True)))\n",
    "        # Add dropout to prevent overfitting\n",
    "        model.add(Dropout(0.3))\n",
    "\n",
    "        # Add third Bidirectional LSTM layer\n",
    "        model.add(Bidirectional(LSTM(32)))\n",
    "\n",
    "        # Output layer with 1 neuron and sigmoid activation for binary classification\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "        # Compile the model with binary cross-entropy loss and Adam optimizer\n",
    "        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['Recall', 'Precision'])\n",
    "\n",
    "        # Define the ModelCheckpoint callback to save the best model\n",
    "        checkpoint_callback = ModelCheckpoint(save_filepath, save_best_only=True, monitor='val_loss', mode='min')\n",
    "\n",
    "        # Train the model\n",
    "        history = model.fit(self.train_signals, self.train_labels, epochs=10, batch_size=64,\n",
    "                            validation_split=0.3, callbacks=[checkpoint_callback])\n",
    "\n",
    "        self.model = model\n",
    "        self.recall = history.history['val_recall']\n",
    "\n",
    "    def evaluate(self):\n",
    "        if self.model is None:\n",
    "            print(\"Model has not been trained yet.\")\n",
    "            return\n",
    "\n",
    "        loss, recall, precision = self.model.evaluate(self.test_signals, self.test_labels)\n",
    "        print(f\"Test loss: {loss:.4f}, Test Recall: {recall:.4f}, Test Precision: {precision:.4f}\")\n",
    "\n",
    "        # Prediction\n",
    "        y_prediction = (self.model.predict(self.test_signals) > 0.5).astype(int)\n",
    "        y_true = self.test_labels\n",
    "\n",
    "        # Plot confusion matrix\n",
    "        cm = confusion_matrix(y_true, y_prediction)\n",
    "        cm_labels = [\"Non-seizure\", \"Seizure\"]\n",
    "        ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=cm_labels).plot(cmap=plt.cm.Blues)\n",
    "        plt.show()\n",
    "\n",
    "        # Plot recall vs. epochs graph\n",
    "        epochs = range(1, len(self.recall) + 1)\n",
    "        plt.plot(epochs, self.recall)\n",
    "        plt.ylabel('Recall')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.legend(['Train', 'Test'], loc='upper left')\n",
    "        plt.show()\n",
    "\n",
    "    @staticmethod\n",
    "    def load_model_from_file(filename):\n",
    "        \"\"\"Load a pre-trained model from a file and return it.\"\"\"\n",
    "        try:\n",
    "            model = load_model(filename)\n",
    "            print(f\"Model loaded from {filename}\")\n",
    "            return model\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model: {e}\")\n",
    "            return None\n",
    "\n"
   ],
   "id": "9e39964aeeda69df"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Initialize and train the model\n",
    "rnn_model = RnnModel(train_signals, train_labels, test_signals, test_labels)\n",
    "rnn_model.train(save_filepath='best_rnn_model.keras')\n",
    "\n",
    "# Later, load the best model into a new instance\n",
    "loaded_model = RnnModel.load_model_from_file('best_rnn_model.keras')\n",
    "\n",
    "# You can then evaluate the loaded model without retraining\n",
    "if loaded_model:\n",
    "    # Create a new RnnModel instance and set its model to the loaded one\n",
    "    new_rnn_model = RnnModel(train_signals, train_labels, test_signals, test_labels)\n",
    "    new_rnn_model.model = loaded_model\n",
    "    new_rnn_model.evaluate()\n"
   ],
   "id": "3e6d239f642acf71"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### convolutional lstm",
   "id": "82cb5a0c124f0373"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#delete\n",
    "class ConvLSTM:\n",
    "    def __init__(self, shape):\n",
    "        self.shape = shape\n",
    "\n",
    "    def hybrid(self):\n",
    "        model = Sequential()\n",
    "        model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=self.shape))\n",
    "        model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "        model.add(Conv1D(filters=128, kernel_size=3, activation='relu'))\n",
    "        model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "        #model.add(Flatten())\n",
    "        model.add(LSTM(128, activation='relu', return_sequences=False))\n",
    "\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['Recall','Precision'])\n",
    "        model.summary()\n",
    "        return model\n"
   ],
   "id": "dcaa446971d7bae1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Prototype model convolutional lstm",
   "id": "4b1360292d2b3c8e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class PrototypeModel:\n",
    "    def __init__(self, shape):\n",
    "        self.shape = shape\n",
    "\n",
    "    def hybrid(self):\n",
    "        model = Sequential()\n",
    "        model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=self.shape))\n",
    "        model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "        model.add(Conv1D(filters=128, kernel_size=3, activation='relu'))\n",
    "        model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "        #model.add(Flatten())\n",
    "        model.add(LSTM(128, activation='relu', return_sequences=False))\n",
    "\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['Recall','Precision'])\n",
    "        model.summary()\n",
    "        return model\n"
   ],
   "id": "7a8bb53de66c4b51"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "p_model = PrototypeModel(train_signals.shape[1:])\n",
    "cnn_lstm = p_model.hybrid()"
   ],
   "id": "60ceaaca55310832"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "callbacks = [ModelCheckpoint('seizure_CNN_LSTM.keras', save_best_only=True)]\n",
    "cnn_lstm_history = cnn_lstm.fit(train_signals, train_labels, epochs=10, batch_size=32,\n",
    "            validation_data=(val_signals,val_labels), callbacks=callbacks)"
   ],
   "id": "842c2a09b6728000"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "b1709d4fc8ca4cb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Plot graph\n",
    "if util.find_spec('matplotlib') is None:\n",
    "    print(\"matplotlib module not available\")"
   ],
   "id": "26a2ad7d729c1b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_summary(history):\n",
    "    epochs = range(1,len(history.history['loss'])+1)\n",
    "    recall = history.history['recall']\n",
    "    precision = history.history['precision']\n",
    "    plt.figure()\n",
    "    plt.plot(epochs, recall, 'bo', label='Recall')\n",
    "    plt.plot(epochs, precision, 'g+', label='Precision')\n",
    "    plt.title('Recall and Precision')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "r_= cnn_lstm_history.history['recall']\n",
    "p_ = cnn_lstm_history.history['precision']"
   ],
   "id": "57e53c4663a62e77"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "convolutional transformer",
   "id": "8f7ff0cb8b0492e5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Transformer encoder layer\n",
    "class Transformer(Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.attention = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.sequential = tf.keras.Sequential([\n",
    "            Dense(ff_dim, activation=\"relu\"),\n",
    "            Dense(embed_dim),\n",
    "        ])\n",
    "        self.normalised1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.normalised2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = Dropout(rate)\n",
    "        self.dropout2 = Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.attention(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.normalised1(inputs + attn_output)\n",
    "        ffn_output = self.sequential(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.normalised2(out1 + ffn_output)\n",
    "\n",
    "class CNN_Transformer:\n",
    "    def __init__(self, input_shape, num_filters=32, kernel_size=3, embed_dim=64, num_heads=4, ff_dim=128, num_classes=1):\n",
    "        self.input_shape = input_shape\n",
    "        self.num_filters = num_filters\n",
    "        self.kernel_size = kernel_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.ff_dim = ff_dim\n",
    "        self.num_classes = num_classes\n",
    "        self.model = self.build_model()\n",
    "\n",
    "    def build_model(self):\n",
    "        inputs = Input(shape=self.input_shape)\n",
    "\n",
    "        # CNN Block: Extract spatial-temporal features\n",
    "        x = Conv1D(filters=self.num_filters, kernel_size=self.kernel_size, activation=\"relu\", padding=\"same\")(inputs)\n",
    "        x = Conv1D(filters=self.num_filters * 2, kernel_size=self.kernel_size, activation=\"relu\", padding=\"same\")(x)\n",
    "        x = MaxPooling1D(pool_size=2)(x)\n",
    "        x = BatchNormalization()(x)\n",
    "\n",
    "        # Additional CNN layers for deeper feature extraction\n",
    "        x = Conv1D(filters=self.num_filters * 4, kernel_size=self.kernel_size, activation=\"relu\", padding=\"same\")(x)\n",
    "        x = MaxPooling1D(pool_size=2)(x)\n",
    "        x = BatchNormalization()(x)\n",
    "\n",
    "        # Flatten for embedding\n",
    "        x = Dense(self.embed_dim)(x)\n",
    "\n",
    "        # Transformer Block\n",
    "        x = Transformer(embed_dim=self.embed_dim, num_heads=self.num_heads, ff_dim=self.ff_dim)(x)\n",
    "\n",
    "        # Global pooling\n",
    "        x = GlobalAveragePooling1D()(x)\n",
    "\n",
    "        # Fully connected layers for classification\n",
    "        x = Dense(64, activation=\"relu\")(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "        outputs = Dense(self.num_classes, activation=\"sigmoid\")(x)\n",
    "\n",
    "        model = Model(inputs=inputs, outputs=outputs)\n",
    "        return model\n",
    "\n",
    "    def compile(self):\n",
    "        self.model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    def fit(self, X_train, y_train, batch_size=32, epochs=10, validation_data=None):\n",
    "        return self.model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=validation_data)\n",
    "\n",
    "    def evaluate(self, X_test, y_test):\n",
    "        return self.model.evaluate(X_test, y_test)\n"
   ],
   "id": "7904a223422eea65"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "pure transformer",
   "id": "904525061e6a79bb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# input_shape = (samples,23, 2560)  # (samples, channels, time steps)\n",
    "\n",
    "# Transformer encoder layer\n",
    "class Transformer(Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.attention = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        # A feed forward network with non linear transformation of features at each neuron\n",
    "        self.sequential = tf.keras.Sequential([\n",
    "            Dense(ff_dim, activation=\"relu\"),\n",
    "            # Each channel is transformed to a dense representation using Dense(embed_dim).\n",
    "            Dense(embed_dim),\n",
    "        ])\n",
    "        self.normalised1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.normalised2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = Dropout(rate)\n",
    "        self.dropout2 = Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.attention(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.normalised1(inputs + attn_output)\n",
    "        ffn_output = self.sequential(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.normalised2(out1 + ffn_output)\n",
    "\n",
    "# Build the model\n",
    "def build_model(input_shape, num_heads=4, ff_dim=128, embed_dim=64, num_classes=1):\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    # Flatten the time series for embedding\n",
    "    x = Dense(embed_dim)(inputs)  # Create embedding\n",
    "    x = Transformer(embed_dim, num_heads, ff_dim)(x)\n",
    "\n",
    "    # Global pooling\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "\n",
    "    # Fully connected output\n",
    "    x = Dense(64, activation=\"relu\")(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    outputs = Dense(num_classes, activation=\"sigmoid\")(x)  # For binary classification\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n"
   ],
   "id": "1d67729de7d78396"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
